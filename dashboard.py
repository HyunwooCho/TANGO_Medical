import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
import nibabel as nib
import io
import torch
import os
import tempfile
import sys
import monai
import pandas as pd
from datetime import datetime
import glob
import zipfile
import io
from pathlib import Path

# Set page config as the first Streamlit command
st.set_page_config(
    page_title="ğŸ©º ì˜ë£Œ ì˜ìƒ AI í•™ìŠµ ë° ë¶„ì„ ì‹œìŠ¤í…œ", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Fix for Streamlit watcher error with torch ---
# Disable file watching to prevent the torch._classes error
if 'STREAMLIT_RUN_PATH' in os.environ:
    import streamlit.watcher.path_watcher
    streamlit.watcher.path_watcher.watch_file = lambda x: None

# Set environment variable to limit file watching
os.environ["STREAMLIT_GLOBAL_WATCHER_MAX_FILES"] = "5000"

# --- Display MONAI version for debugging ---
st.sidebar.info(f"MONAI version: {monai.__version__}")

# --- Utility Classes ---
class MedicalImageLoader:
    def __init__(self):
        from monai.transforms import (
            Compose, 
            ScaleIntensity, 
            ResizeWithPadOrCrop
        )
        
        self.transforms = Compose([
            ScaleIntensity(),
            ResizeWithPadOrCrop((128, 128, 128))
        ])
    
    def load_image(self, uploaded_file):
        try:
            # Read the uploaded file
            content = uploaded_file.getvalue()
            
            # Create a temporary file to store the uploaded content
            with tempfile.NamedTemporaryFile(suffix='.nii.gz', delete=False) as temp_file:
                temp_path = temp_file.name
                temp_file.write(content)
                
            # Load with nibabel
            img = nib.load(temp_path)
            img_data = img.get_fdata()
            
            # Clean up the temporary file
            os.remove(temp_path)
            
            # Get middle slice for display
            mid_slice = img_data.shape[2] // 2
            slice_data = img_data[:, :, mid_slice].astype(np.float32)
            
            # Normalize for display
            slice_data = (slice_data - slice_data.min()) / (slice_data.max() - slice_data.min() + 1e-8)
            
            # Process for model
            processed_data = img_data.astype(np.float32)
            processed_data = (processed_data - processed_data.min()) / (processed_data.max() - processed_data.min() + 1e-8)
            processed_volume = torch.from_numpy(processed_data[np.newaxis, np.newaxis, ...])
            
            # Apply size transformation
            processed_volume = self.transforms(processed_volume)
            
            return slice_data, processed_volume
        except Exception as e:
            st.error(f"Error loading image: {str(e)}")
            return None, None

class SimpleSwinUNETR(torch.nn.Module):
    """A simplified wrapper for SwinUNETR that handles version compatibility"""
    def __init__(self):
        super().__init__()
        try:
            # Try importing without img_size first (newer versions)
            from monai.networks.nets import SwinUNETR
            try:
                self.model = SwinUNETR(
                    in_channels=1,
                    out_channels=2,
                    feature_size=48,
                    use_checkpoint=True
                )
                st.sidebar.success("MONAI 1.5+ detected - initialized model without img_size")
            except TypeError:
                # If that fails, try with img_size (older versions)
                self.model = SwinUNETR(
                    img_size=(128, 128, 128),
                    in_channels=1,
                    out_channels=2,
                    feature_size=48,
                    use_checkpoint=True
                )
                st.sidebar.success("MONAI <1.5 detected - initialized model with img_size")
        except Exception as e:
            st.error(f"Error initializing SwinUNETR: {str(e)}")
            # Create a dummy model as a fallback
            self.model = torch.nn.Sequential(
                torch.nn.Conv3d(1, 16, kernel_size=3, padding=1),
                torch.nn.ReLU(),
                torch.nn.Conv3d(16, 2, kernel_size=3, padding=1)
            )
            st.warning("âš ï¸ Using fallback model - SwinUNETR initialization failed")
    
    def forward(self, x):
        return self.model(x)

class SwinUNETRModel:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def get_model(self):
        try:
            # Use our version-agnostic wrapper
            model = SimpleSwinUNETR().to(self.device)
            model.eval()
            return model
        except Exception as e:
            st.error(f"Error loading model: {str(e)}")
            return None

class GradCAM:
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.gradients = None
        self.activations = None
        
    def save_gradient(self, grad):
        self.gradients = grad.detach()
    
    def backward_hook(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()
        
    def forward_hook(self, module, input, output):
        self.activations = output.detach()
        
    def apply_gradcam(self, image_tensor):
        if self.model is None:
            return np.zeros((128, 128))
        
        # If the model is our simplified wrapper, use the actual model
        if hasattr(self.model, 'model'):
            target_model = self.model.model
        else:
            target_model = self.model
            
        try:
            # Register hooks on a suitable layer
            # Try different approaches to find a good target layer
            target_layer = None
            
            # Approach 1: Look for swinViT.layers
            if hasattr(target_model, 'swinViT') and hasattr(target_model.swinViT, 'layers'):
                target_layer = target_model.swinViT.layers[-1]
            
            # Approach 2: Look for a convolutional layer
            if target_layer is None:
                for name, module in target_model.named_modules():
                    if isinstance(module, torch.nn.Conv3d):
                        target_layer = module
                        break
            
            # If still no target layer, create a dummy one
            if target_layer is None:
                st.warning("âš ï¸ Couldn't find suitable layer for Grad-CAM, using dummy heatmap")
                return np.random.uniform(0, 1, (128, 128))
                
            # Register hooks
            forward_handle = target_layer.register_forward_hook(self.forward_hook)
            backward_handle = target_layer.register_backward_hook(self.backward_hook)
            
            # Forward pass
            image_tensor = image_tensor.to(self.device)
            
            # Create a clone that requires grad
            input_tensor = image_tensor.clone().detach().requires_grad_(True)
            
            output = self.model(input_tensor)
            
            # For binary classification, take the positive class score
            target_score = output[0, 1]  # Targeting class 1 (positive class)
            
            # Backward pass
            self.model.zero_grad()
            target_score.backward()
            
            # Get gradients and activations
            if self.gradients is None or self.activations is None:
                # If hooks didn't work, return random heatmap
                st.warning("Couldn't generate Grad-CAM - hooks didn't capture gradients or activations")
                return np.random.uniform(0, 1, (128, 128))
                
            # Calculate weights
            pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3, 4])
            
            # Weight the channels by corresponding gradients
            cam = torch.zeros(self.activations.shape[2:], dtype=torch.float32).to(self.device)
            
            # Multiply each weight with its conv output and then sum
            for i, w in enumerate(pooled_gradients):
                cam += w * self.activations[0, i, :, :, :]
                
            # Apply ReLU to the heatmap
            cam = torch.relu(cam)
            
            # Normalize
            if torch.max(cam) > 0:
                cam = cam / torch.max(cam)
            
            # Convert to numpy
            cam = cam.cpu().detach().numpy()
            
            # Get middle slice for 2D visualization
            mid_slice = cam.shape[2] // 2
            heatmap_2d = cam[:, :, mid_slice]
            
            # Clean up hooks
            forward_handle.remove()
            backward_handle.remove()
            
            return heatmap_2d
            
        except Exception as e:
            st.error(f"Error generating Grad-CAM: {str(e)}")
            # Return a random heatmap as a placeholder
            return np.random.uniform(0, 1, (128, 128))


# --- Streamlit UI ---
st.title("ğŸ©º ì˜ë£Œ ì˜ìƒ AI ë¶„ì„ ì‹œìŠ¤í…œ")
st.sidebar.header("ğŸ” ì„¤ì •")

# Add model selection for flexibility
model_type = st.sidebar.selectbox(
    "ëª¨ë¸ ì„ íƒ",
    ["Swin UNETR"]
)

# --- Data Loader ì´ˆê¸°í™” ---
loader = MedicalImageLoader()

# --- Swin UNETR ëª¨ë¸ ë¡œë“œ ---
with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
    model_instance = SwinUNETRModel()
    model = model_instance.get_model()
    device = model_instance.device

if model is not None:
    st.sidebar.success("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
    # --- Grad-CAM ì´ˆê¸°í™” ---
    gradcam = GradCAM(model, device)

    # --- íŒŒì¼ ì—…ë¡œë“œ ---
    uploaded_file = st.sidebar.file_uploader("ğŸ“¤ NIfTI(.nii, .nii.gz) íŒŒì¼ ì—…ë¡œë“œ", type=["nii", "nii.gz"])

    # If no file is uploaded, provide a demo option
    if uploaded_file is None:
        use_demo = st.sidebar.checkbox("ë°ëª¨ ì´ë¯¸ì§€ ì‚¬ìš©")
        if use_demo:
            st.info("ë°ëª¨ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (ë¬´ì‘ìœ„ ìƒì„± ë°ì´í„°)")
            # Create a demo volume
            demo_volume = torch.rand(1, 1, 128, 128, 128)
            demo_slice = demo_volume[0, 0, :, :, 64].numpy()
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“· ë°ëª¨ ì´ë¯¸ì§€")
                fig1, ax1 = plt.subplots(figsize=(5, 5))
                ax1.imshow(demo_slice, cmap="gray")
                ax1.axis("off")
                st.pyplot(fig1)

            with col2:
                st.subheader("ğŸ” Grad-CAM ë¶„ì„ (ë°ëª¨)")
                with st.spinner("Grad-CAM ë¶„ì„ ì¤‘..."):
                    heatmap = gradcam.apply_gradcam(demo_volume)

                    fig2, ax2 = plt.subplots(figsize=(5, 5))
                    ax2.imshow(demo_slice, cmap="gray")
                    ax2.imshow(heatmap, cmap="jet", alpha=0.5)
                    ax2.axis("off")
                    st.pyplot(fig2)
                    
                    st.success("âœ… Grad-CAM ë¶„ì„ ì™„ë£Œ!")
    
    elif uploaded_file is not None:
        with st.spinner("ì´ë¯¸ì§€ ë¡œë”© ì¤‘..."):
            slice_image, processed_volume = loader.load_image(uploaded_file)
        
        if slice_image is not None:
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“· ì›ë³¸ ì´ë¯¸ì§€")
                fig1, ax1 = plt.subplots(figsize=(5, 5))
                ax1.imshow(slice_image, cmap="gray")
                ax1.axis("off")
                st.pyplot(fig1)

            with col2:
                st.subheader("ğŸ” Grad-CAM ë¶„ì„")
                with st.spinner("Grad-CAM ë¶„ì„ ì¤‘..."):
                    heatmap = gradcam.apply_gradcam(processed_volume)

                    fig2, ax2 = plt.subplots(figsize=(5, 5))
                    ax2.imshow(slice_image, cmap="gray")
                    ax2.imshow(heatmap, cmap="jet", alpha=0.5)
                    ax2.axis("off")
                    st.pyplot(fig2)
                    
                    st.success("âœ… Grad-CAM ë¶„ì„ ì™„ë£Œ!")
            
            # Add 3D volume visualization option
            if st.checkbox("3D ë³¼ë¥¨ ì‹œê°í™” (ìŠ¬ë¼ì´ìŠ¤ ë·°)"):
                slice_idx = st.slider("ìŠ¬ë¼ì´ìŠ¤ ì„ íƒ", 0, processed_volume.shape[4]-1, processed_volume.shape[4]//2)
                
                fig3, ax3 = plt.subplots(figsize=(8, 8))
                volume_data = processed_volume.squeeze().cpu().numpy()
                selected_slice = volume_data[:, :, slice_idx]
                ax3.imshow(selected_slice, cmap="gray")
                ax3.axis("off")
                ax3.set_title(f"ìŠ¬ë¼ì´ìŠ¤ {slice_idx}")
                st.pyplot(fig3)
else:
    st.error("âš ï¸ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. MONAI ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    st.info("ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”: `pip install monai --upgrade`")

# Add tabs for additional information
tab1, tab2, tab3 = st.tabs(["ê²°ê³¼ í•´ì„", "ëª¨ë¸ ì •ë³´", "ì‚¬ìš© ë°©ë²•"])

with tab1:
    st.subheader("ê²°ê³¼ í•´ì„")
    st.write("""
    - **ë¶‰ì€ìƒ‰ ì˜ì—­**: ëª¨ë¸ì´ ì§„ë‹¨ì— ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•œ ì˜ì—­
    - **íŒŒë€ìƒ‰ ì˜ì—­**: ëª¨ë¸ì´ ìƒëŒ€ì ìœ¼ë¡œ ëœ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í•œ ì˜ì—­
    
    ìœ„ íˆíŠ¸ë§µì€ ëª¨ë¸ì´ íŒë‹¨ì„ ë‚´ë¦¬ëŠ” ë° ìˆì–´ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ë³¸ ì˜ì—­ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    ì´ëŠ” ëª¨ë¸ì˜ ê²°ì •ì— ëŒ€í•œ ì„¤ëª… ê°€ëŠ¥ì„±(Explainable AI)ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)

with tab2:
    st.subheader("Swin UNETR ëª¨ë¸ ì •ë³´")
    st.write("""
    **Swin UNETR**ëŠ” ì˜ë£Œ ì˜ìƒ ë¶„í• ì„ ìœ„í•œ Vision Transformer ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤:
    
    - **ì•„í‚¤í…ì²˜**: ì¸ì½”ë”ë¡œ Swin Transformerë¥¼, ë””ì½”ë”ë¡œ CNN êµ¬ì¡°ë¥¼ ì‚¬ìš©
    - **ì¥ì **: ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ì—¬ ë³µì¡í•œ ì˜ë£Œ ì˜ìƒ ë¶„ì„ì— ì í•©
    - **ì„±ëŠ¥**: ì—¬ëŸ¬ ì˜ë£Œ ì˜ìƒ ë°ì´í„°ì…‹ì—ì„œ ìµœì²¨ë‹¨ ì„±ëŠ¥ ë‹¬ì„±
    - **ë…¼ë¬¸**: "Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images" (MICCAI 2021)
    """)

with tab3:
    st.subheader("ì‚¬ìš© ë°©ë²•")
    st.write("""
    1. ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ NIfTI í˜•ì‹ì˜ ì˜ë£Œ ì˜ìƒ íŒŒì¼(.nii ë˜ëŠ” .nii.gz)ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    2. ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ê³  Grad-CAM ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    3. ì›ë³¸ ì´ë¯¸ì§€ì™€ Grad-CAM íˆíŠ¸ë§µì´ ë‚˜ë€íˆ í‘œì‹œë©ë‹ˆë‹¤.
    4. 3D ë³¼ë¥¨ ì‹œê°í™” ì˜µì…˜ì„ ì„ íƒí•˜ì—¬ ë‹¤ì–‘í•œ ìŠ¬ë¼ì´ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    5. 'ê²°ê³¼ í•´ì„' íƒ­ì—ì„œ íˆíŠ¸ë§µ í•´ì„ ë°©ë²•ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)

# Add debug information
with st.sidebar.expander("ğŸ“Š ë””ë²„ê·¸ ì •ë³´"):
    st.write(f"Python ë²„ì „: {sys.version}")
    st.write(f"PyTorch ë²„ì „: {torch.__version__}")
    st.write(f"MONAI ë²„ì „: {monai.__version__}")
    st.write(f"ë””ë°”ì´ìŠ¤: {device}")

# Add a footer
st.markdown("---")
st.markdown("Â© 2025 ì˜ë£Œ ì˜ìƒ AI ë¶„ì„ ì‹œìŠ¤í…œ")